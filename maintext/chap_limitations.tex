\chapter{Limitations}

While there are a lot of studies that showcase significant improvement in disease classification through MRI scans, it is necessary to accept that several limitations exist before any clinical utility can be recognized. These limitations arise from the inherent variability from multiple sources and the different acquisition protocols and systems, from the experiment architectures and the intricacies of different approaches, along with the heterogeneity of the disease. Understanding these limitations is essential for future research and building more robust, reliable and clinically translatable systems.

\section{Data Leakage}

The primary limitation that has been researched in the field of medical imaging and machine learning is that of data leakage, which presents a serious obstacle for clinical translation. As documented in \cite{wenConvolutionalNeuralNetworks2020}, nearly half of the studies between 2017 and 2019 failed to separate same subject data from contaminating the testing set. This leakage creates artificially inflated accuracy metrics that do not reflect generalized diagnostic capability.

The most common cause of data leakage seems to happen through ``slice-level'' splitting, where MRI slices from the same patient are distributed across both sets (training, testing). This leads to model overfitting, a poor indicator of clinical performance and generalization or broader pattern recognition.

Pulido-Salalgado et al. (2025) \cite{youngDataLeakageDeep2025} demonstrated this effect empirically within a single study, showing a 28-percentage-point accuracy drop when switching from slice-wise to subject-wise splitting. The magnitude of this drop---equivalent to the difference between a highly promising diagnostic tool and one barely exceeding chance---illustrates why methodological rigor is non-negotiable.

Furthermore, data contamination can persist in other forms. Longitudinal studies usually include data from the same subject at different disease stages, and if these visits are not carefully tracked, temporal correlations can produce data leakage. Similarly, datasets that aggregate data from multiple acquisition sites may include the same patient scanned at different facilities. In Young (2025) \cite{youngDataLeakageDeep2025}, it was found that only 4.5\% of published studies implemented the complete ``methodological triad'' of subject-wise splitting, external validation, and confounder control, suggesting that the field's reported performance metrics are systematically optimistic.

\section{The Accuracy Paradox and Class Imbalance}

The dataset employed in this study, drawn from the Alzheimer's Disease Neuroimaging Initiative (ADNI), exhibits significant class imbalance typical of clinical populations. Healthy Controls (HC) and Mild Cognitive Impairment (MCI) cases substantially outnumber confirmed Alzheimer's Disease cases. This imbalance renders overall accuracy an unreliable and potentially misleading performance metric, a phenomenon termed the ``accuracy paradox'' by Dubray et al. (2024) \cite{tongClassBalancedDeepLearning2024}.

The dataset employed in this study also exhibits significant class imbalance as can be seen in Figure~\ref{fig:class_imbalance}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/limitations_class_imbalance.png}
    \caption{Class distribution showing the imbalance in the dataset.}
    \label{fig:class_imbalance}
\end{figure}

This imbalance, as has already been mentioned, renders accuracy as an unreliable metric and more precise and robust metrics have been proposed (F1-score, AUC, MCC, etc.). The phenomenon of imbalance leading to misleading performance has been termed as ``accuracy paradox'' in Dubray et al. (2024) \cite{tongClassBalancedDeepLearning2024}.

To frame this in practical terms, imagine a medical test that correctly identifies 9 out of 10 people, but only because it tells everyone they are healthy. The 1 in 10 patients with Alzheimer's Disease would receive false reassurance, potentially delaying critical interventions during the narrow window when treatments are most effective. This scenario illustrates why clinical AI systems must be evaluated on metrics beyond raw accuracy.

For this reason, the results presented in this thesis prioritize sensitivity, specificity, F1-score, and Area Under the Receiver Operating Characteristic curve (AUC-ROC) alongside accuracy. We employed data augmentation using the MONAI (open source framework), weighted loss functions, and balanced sampling to address class imbalance during training. Moreover, additional frameworks like Grad-CAM can provide interpretability and were used in this study to assess model pattern recognition and be able to provide clinicians with an explanation of what the model evaluates as most important.

Nevertheless, residual bias toward the majority class likely persists. The model may still exhibit lower confidence and higher error rates when classifying true AD cases compared to healthy controls---a bias that would disproportionately affect the patients who most need accurate diagnosis.

\section{Domain Shift and Generalizability}

Most of the models where research is implemented are using curated datasets from research-grade MRI data (ADNI, OASIS) which employ standardized acquisition protocols across different sites fail when deployed on data acquired under different circumstances. While the standardized process reduces confounding variables during development, they introduce a critical limitation known as ``domain shift.''

\subsection{Sources of Domain Shift in MRI Data}

Magnetic field strength significantly impacts image resolution and for higher-field strength images contain a better signal-to-noise ratio, but can also amplify certain artifacts. A model thus that has been trained on specific images will fail to accurately predict correctly because of unfamiliar noise patterns or reduced visibility of subtle atrophy.

Acquisition sequence parameters---including echo time, repetition time, flip angle, and slice thickness---vary across institutions and protocols. What constitutes a ``standard'' T1-weighted sequence differs between research consortia and clinical departments. The model has learned one version of normal and may misinterpret technically valid but differently acquired images.

Diogo et al. (2022) \cite{diogoEarlyDiagnosisAlzheimers2022} documented that models trained on research-grade images from one set frequently fail when applied to clinical-grade images from hospital settings. Dinsdale et al. (2024) \cite{turrisiDeepLearningbasedAlzheimers2024a} found that accuracy dropped to 71\% when models were tested on external datasets with different scanner configurations, compared to training data performance.

Critically, we did not perform external validation on a completely independent cohort (such as training on ADNI and testing on OASIS), meaning our reported metrics almost certainly overestimate real-world clinical performance.

\section{Shortcut Learning and Region of Interest Bias}

Another critical limitation is that deep learning models can learn shortcuts that are contained in medical images instead of actually learning the underlying conditions of the disease. This is called ``shortcut learning.'' This is also mentioned in interpretability of deep neural networks as a way to ensure model understanding at the programming phase and also enhance clinician trust.

\subsection{Skull-Stripping Artifacts}

Preprocessing pipelines usually include a step for non-brain tissue (skull stripping) to focus the model directly on the brain tissue and its structure. However, as noted in Wen et al. (2020) \cite{wenConvolutionalNeuralNetworks2020}, imperfect skull stripping can leave informative artifacts. If there is a difference in the stripping algorithm between healthy brains and atrophic brains---for example, leaving more residual scalp on atrophic brains where the boundary of brain-skull is less distinct---then it is likely that the model may learn to classify images based on preprocessing residuals rather than brain tissue characteristics.

The gap between brain surface and inner skull table increases with cortical atrophy, a hallmark of Alzheimer's Disease. If skull-stripping is incomplete, this enlarged cerebrospinal fluid space becomes visible as a classification cue. The model correctly identifies Alzheimer's cases, but for the wrong reason---it has learned to detect atrophy artifacts rather than analyze hippocampal or cortical tissue directly.

\subsection{Biological Plausibility of Learned Features}

In our experiments, we used Gradient-weighted Class Activation Mapping (Grad-CAM) for model interpretability, generating heatmaps indicating image regions that influenced the model's decision. However, it should be noted that interpretability methods are not validation methods, since a model can focus on ventricular enlargement which is a secondary issue but ignore the primary underlying pathology while still classifying a majority of samples correctly.

Young et al. (2025) \cite{youngDataLeakageDeep2025} found that while 18.2\% of studies used interpretability methods, only 12.5\% of Grad-CAM implementations validated that highlighted regions corresponded to known neuropathological sites. This interpretability-validation chasm means that impressive heatmaps clustering around ``reasonable'' brain regions do not confirm that the model has learned disease-relevant features. Clinicians cannot trust explanations that lack rigorous validation against established pathological patterns. Figure~\ref{fig:gradcam_implementation} shows a Grad-CAM implementation along with the code.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/limitations_gradcam.png}
    \caption{Grad-CAM visualization showing original image, heatmap, and overlay.}
    \label{fig:gradcam_implementation}
\end{figure}

\begin{lstlisting}[language=Python, caption={Grad-CAM visualization implementation}]
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget

def visualize_gradcam(model, img_tensor, true_label, target_layer):
    """Display GradCAM visualization for a single image."""
    img_tensor = img_tensor.to(device)

    # Create GradCAM object
    cam = GradCAM(model=model, target_layers=[target_layer])

    # Get prediction
    model.eval()
    with torch.no_grad():
        probs = F.softmax(model(img_tensor), dim=1)[0]
        pred = probs.argmax().item()

    # Generate GradCAM heatmap
    grayscale_cam = cam(input_tensor=img_tensor,
                        targets=[ClassifierOutputTarget(pred)])[0]

    # Prepare image for visualization
    img = img_tensor.cpu().squeeze().numpy()

    # Normalize image to [0, 1] range
    img_normalized = (img - img.min()) / (img.max() - img.min() + 1e-8)

    # Convert grayscale to RGB
    img_rgb = np.stack([img_normalized]*3, axis=-1).astype(np.float32)

    # Create overlay
    overlay = show_cam_on_image(img_rgb, grayscale_cam, use_rgb=True)
\end{lstlisting}

\section{Ground Truth Uncertainty and Diagnostic Heterogeneity}

Another important limitation is the reliability of diagnostic labels that are used for training. Machine learning models are upper-limited by the labels and data they are given, and it seems that clinical diagnosis of Alzheimer's Disease is imperfect. Wang et al. (2025) found that 71\% of patients with dementia at autopsy had multiple coexisting pathologies, but only 67\% had been clinically diagnosed with AD alone.

The same study demonstrated that when models were trained only on confirmed neuropathologically diagnoses instead of clinical labels, accuracy changed substantially---both for AD (84.4\%) and vascular dementia (83.9\%) but lower for Lewy body dementia (62.3\%). This suggests that for different dementia subtypes there are different challenges for classification and that incorporating them under imprecise clinical labels degrades performance for all categories.

\section{Disease Heterogeneity as an Intrinsic Limitation}

Alzheimer's disease itself shows substantial phenotypic heterogeneity. In Young et al. (2018) \cite{youngUncoveringHeterogeneityTemporal2018}, at least three distinct disease progression patterns were identified. Patients may present with predominantly hippocampal atrophy, predominantly cortical atrophy, or mixed patterns, with different sequences of regional involvement over time.

In another study, Kumar et al. (2024) \cite{kumarAlzheimerDisease2025} showed that even greater heterogeneity was found at more severe disease stages. Patients showcased distinct patterns of neurodegeneration, tau accumulation, and amyloid deposition. This presents a limitation where a binary classification between AD and control subjects might be inadequate since the disease manifests through multiple pathways with patient-specific trajectories.

\section{Transparency, Reproducibility, and the Black Box Problem}

Deep learning models operate as ``black boxes,'' extracting features through millions of parameters without providing human-interpretable explanations for their decisions. Jo et al. (2019) emphasized that this opacity is particularly problematic for medical applications, where clinicians must justify diagnostic decisions to patients, families, and institutional review processes.

Reproducibility presents a related challenge. Dinsdale et al. (2024) \cite{turrisiDeepLearningbasedAlzheimers2024} documented that none of the state-of-the-art studies they examined were fully reproducible. Authors rarely provide complete code, and critical implementation details---data augmentation strategies, exact model architectures, hyperparameter values, random seeds---are inconsistently reported. Sample sizes vary from 170 to 1,662 participants across studies, making cross-study benchmarking nearly impossible. Our own results, while documented as thoroughly as practical, may prove difficult to reproduce exactly due to inherent stochasticity in neural network training.

\section{Limitations of Single-Modality Analysis}

This study relies exclusively on structural MRI data, yet clinical diagnosis of AD integrates multiple information sources. In Leming et al. (2023) \cite{lemingChallengesImplementingComputeraided2023}, it is mentioned that at least five modalities exist: structural imaging, functional imaging (PET, fMRI), cerebrospinal fluid biomarkers, genetic testing, and neuropsychological assessment. Relying solely on MRI can negatively impact information that can be leveraged for better model predictions.

However, even though the potential benefits of integrating multiple sources of information are documented \cite{razaAdvancementsDeepLearning2025, golrizkhatamiChallengesIntegrativeDisease2020} and the combination of more modalities could improve accuracy, this can also introduce other complications. Data missingness, variable protocols, and dataset interoperability can increase model complexity and might compromise interpretability.

\section{Summary of Limitations}

The limitations presented in this chapter suggest that the metrics in our results represent upper bounds on clinical utility rather than deployment accuracy. Issues like data leakage may inflate accuracy. Class imbalance renders accuracy as misleading and masks reduced sensitivity for underrepresented classes. Domain shift implies that model performance will degrade if it encounters images from different scanners, protocols, or patient populations.

Shortcut learning means we cannot be confident that the model has learned disease-relevant features rather than confounding artifacts. Ground truth uncertainty implies that our training labels are themselves noisy, introducing irreducible error into the learning process. Disease heterogeneity suggests that the binary classification paradigm may be fundamentally mismatched to the biological reality of Alzheimer's Disease.

These limitations do not invalidate the work presented in this thesis, but they do circumscribe its claims. The model demonstrates proof-of-concept that convolutional neural networks can learn discriminative features from brain MRI data, but substantial additional work---external validation, prospective clinical trials, integration with existing diagnostic workflows---would be required before any clinical deployment could be contemplated. Future research should prioritize the ``methodological triad'' identified by Young et al. (2025): rigorous subject-level data splitting, external validation on independent cohorts, and systematic confounder control. Only through such rigorous methodology can the field move from promising laboratory results toward genuinely useful clinical tools.
