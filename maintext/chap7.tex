\chapter{Challenges in Dementia Image Analysis and Classification}

\section{Imbalanced Data}

It is well understood from the literature that the performance of a machine learning (ML) model is upper bounded by the quality of the data \cite{jainOverviewImportanceData2020}. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection \cite{johnsonSurveyDeepLearning2019a}.

In this problem, the majority of the sample data are labeled as one class and only a few samples are labeled as another class. This creates an imbalance in the dataset, and it is hard to solve because common classification algorithms are not designed to face these sorts of data \cite{niazClassImbalanceProblems2022}. In such a problem, almost all the examples are labeled as one class, while far fewer examples are labeled as the other class, usually the more important class. In this case, standard machine learning algorithms tend to be overwhelmed by the majority class and ignore the minority class since traditional classifiers seek accurate performance over a full range of instances \cite{guoClassImbalanceProblem2008}. In ML problems, differences in prior class probabilities—or class imbalances—have been reported to hinder the performance of standard classifiers such as decision trees \cite{japkowiczClassImbalanceProblem2002}.

When dealing with imbalanced datasets, standard evaluation metrics like accuracy can be misleading, as they are often biased toward the majority class. For a more robust and insightful assessment of classifier performance, it is crucial to employ metrics specifically designed to handle skewed class distributions. Based on \cite{japkowiczAssessmentMetricsImbalanced2013}, these can be divided into two primary categories: \textbf{Threshold Metrics} and \textbf{Ranking Methods}.

\subsection{Threshold Metrics}

Threshold metrics evaluate a classifier's performance at a fixed operational point. They provide a snapshot of performance but assume that the class distribution and error costs are known and constant.

\paragraph{Sensitivity and Specificity}
These metrics evaluate performance on each class independently.

\begin{itemize}
    \item \textbf{Sensitivity (Recall, True Positive Rate):} Proportion of actual positive instances correctly identified.
    \item \textbf{Specificity:} Proportion of actual negative instances correctly identified.
\end{itemize}

\paragraph{Precision and Recall}
Widely used in information retrieval, these focus on the positive class.

\begin{itemize}
    \item \textbf{Precision:} Proportion of predicted positives that are true positives.
    \item \textbf{Recall:} Equivalent to sensitivity.
\end{itemize}

\paragraph{Combined Metrics}
To simplify evaluation, the following metrics combine sensitivity, specificity, precision, and recall:

\begin{itemize}
    \item \textbf{G-Mean:} Geometric mean of sensitivity and specificity, balancing performance across classes.
    \item \textbf{F-Measure:} Weighted harmonic mean of precision and recall, widely used in text categorization.
\end{itemize}

\subsection{Ranking Methods and Metrics}

Ranking methods evaluate performance across all decision thresholds, useful when class distribution or error costs vary.

\paragraph{ROC Analysis}
Plots True Positive Rate vs. False Positive Rate across thresholds. A curve near the top-left corner indicates a strong classifier. Particularly effective for imbalanced data.

\paragraph{AUC}
Area Under the ROC Curve; represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative one.

\paragraph{Precision--Recall Curves}
Show precision--recall trade-offs across thresholds and are preferred for highly imbalanced datasets.

\medskip
In conclusion, evaluating classifiers on imbalanced datasets requires combining threshold metrics with ranking-based metrics. While threshold metrics evaluate specific operational conditions, ranking methods such as ROC and PR curves provide a more holistic understanding of classifier performance.

\section{Limited Datasets}

Big data and labels are not always available. Often we only have limited labeled data, such as medical images requiring expert annotation. ML with limited data poses major challenges. Domain shift is a critical issue in few-shot learning when training and testing domains differ \cite{yaoMachineLearningLimited2021}. Few datasets with limited data exist to train ML models \cite{merkinMachineLearningArtificial2022}, and robust models are hindered by the lack of large annotated datasets \cite{martinInterpretableMachineLearning2023b}.

A recent study reports a dementia progression ML model built using a ``limited, real-world clinical dataset,'' demonstrating that dataset scarcity harms model generalizability and clinical applicability \cite{huangEstablishingMachineLearning2024}. Similarly, a comprehensive review highlights technical barriers and challenges in ML applications for dementia, particularly data scarcity and dataset diversity constraints \cite{wangUnderstandingMachineLearning2024}.

\subsection{Context and Challenges}

Recent research consistently identifies the lack of large-scale, heterogeneous datasets (neuroimaging, biomarkers, clinical records) as a bottleneck for accurate and generalizable dementia ML models. Data limitations affect early diagnosis, subtype prediction, and progression tracking.

\section{Small Sample Sizes}

The combination of DL image analysis and large-scale imaging datasets presents major opportunities, yet barriers such as data availability, interpretability, and logistical constraints persist \cite{ChallengesMachineLearning2022}. Further challenges include generalization, lack of gold standards, privacy concerns, and safety \cite{aljuhaniUseArtificialIntelligence2024}. Studies emphasize the scarcity of sample-size determination methodologies for ML in medical imaging \cite{SampleSizeDeterminationMethodologies2019}. A meta-analysis shows that smaller studies often report inflated diagnostic accuracy, indicating systematic bias \cite{luSmallStudyEffects2022}.

\section{Image Quality and Variability}

MRI intensities are acquired in arbitrary units, making scans non-comparable across sites. Even after normalization, inter-scan variability remains due to scanner effects and technical artifacts \cite{benzingerRegionalVariabilityImaging2013}. Variance in volumetric measures can differ by up to 10× depending on acquisition conditions, influenced by scanner hardware and tissue contrast differences \cite{kruggelImpactScannerHardware2010}. Additional work quantifies scanner make, model, and protocol variability in image quality, which limits generalizability of CNNs across sites \cite{monteroIntraInterscannerCT2025, bhosekarReviewDeepLearningbased}.

\section{Interpretability in Medical Applications}

ML models in medicine must be interpretable, shareable, reproducible, and accountable. Black-box models are often accurate but face regulatory resistance. Interpretable approaches (kernel methods, prototype-based learning, deep kernel models) can provide clinically meaningful insights. Other key components include fairness, uncertainty quantification, and privacy-preserving collaborative learning such as federated learning and synthetic data via diffusion models \cite{begumbektasMachineLearningMedicine2025}. Explainable AI enhances transparency and ethical compliance, though challenges remain in workflow integration and regulatory alignment \cite{SurveyExplainableArtificial2024}. ML has transformed healthcare across applications, though ethical concerns such as bias, privacy, and legal issues remain \cite{MachineLearningHealthcare}. Challenges also persist in skin cancer detection due to data imbalance, interpretability limits, and model generalization difficulties \cite{ReviewApplicationsMachine}.

\section{Computational Costs}

Deep learning progress correlates strongly with rising compute demands, which are becoming economically and environmentally unsustainable \cite{thompsonComputationalLimitsDeep2022}. Efficient deep learning research explores model compression, optimization, and hardware acceleration. Compression techniques such as pruning, quantization, and knowledge distillation can reduce model size by over 100× while maintaining accuracy \cite{PDFEfficientDeep2025}. Lightweight models are essential for deployment on resource-constrained devices \cite{liuLightweightDeepLearning2024}. Architectures such as MobileNet and EfficientNet address this need while sustaining accuracy \cite{nampalleDeepMediXDeepLearningDriven2023}. Transfer learning reduces training costs, improving performance with limited data \cite{dashTransferLearningBased2024}. Pruning and knowledge distillation remain critical for shrinking large models \cite{gouKnowledgeDistillationSurvey2021, vaderaMethodsPruningDeep2022, hoeflerSparsityDeepLearning}. Additional work applies these techniques to Alzheimer’s MRI classification \cite{liClassificationAlzheimersDisease2022}.

\section{Data Distillation}

Data distillation provides a promising avenue for efficient and secure sharing of medical imaging datasets. Instead of sharing full datasets, distilled representations can preserve modeling performance while reducing data volume. Recent investigations show that a small, representative set of distilled images can achieve near-equivalent model performance, demonstrating potential for scalable clinical collaboration \cite{liDatasetDistillationMedical2025}.
