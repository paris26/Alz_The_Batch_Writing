\chapter{Challenges in Dementia Image Analysis and Classification}

\section{Imbalanced Data}

The performance of any machine learning model is fundamentally limited by the quality of its data \cite{jainOverviewImportanceData2020}. A critical challenge that can degrade data quality is class imbalance, which occurs when one class contains many more sample than another - for example medical applications or fraud detection - \cite{johnsonSurveyDeepLearning2019}.
Thi imbalnce therefore represents a major challenge since since standard ML algorithms are designed to maximize overall accuracy \cite{niazClass2022}. When trained on imbalanced data they tend to show bias towards the majority class while placing less significance to the minority class - often the class of most significance , since detecting the presence of disease is the primary interest- \cite{guoClassImbalanceProblem2008}. This performance degradation is documented , in decision tree classifiers \cite{japkowiczClassImbalanceProblem2002}. 

When dealing with imbalanced datasets, standard evaluation metrics like accuracy can be misleading, as they are often biased toward the majority class. For a more robust and insightful assessment of classifier performance, it is crucial to employ metrics specifically designed to handle skewed class distributions. Based on \cite{japkowiczAssessmentMetricsImbalanced2013}, these can be divided into two primary categories: \textbf{Threshold Metrics} and \textbf{Ranking Methods}.

\subsection{Threshold Metrics}

Threshold metrics evaluate a classifier's performance at a fixed operational point. They provide a snapshot of performance but assume that the class distribution and error costs are known and constant.

\paragraph{Sensitivity and Specificity}
These metrics evaluate performance on each class independently.

\begin{itemize}
    \item \textbf{Sensitivity (Recall, True Positive Rate):} The number of positive instances that are correctly identified as in Eq.~\eqref{eq:sensitivity}
    \item \textbf{Specificity:} Negative instances that are correctly identified as in Eq.~\eqref{eq:specificity}
\end{itemize}

\paragraph{Precision and Recall}
These two metrics are the quantification of precision and recall 

\begin{itemize}
    \item \textbf{Precision:} Precision is used to show the proportion of examples that are correctly identified Eq. \eqref{precision}
    \item \textbf{Recall:} The ratio of treu positives over the actual positives ( Eq \eqref{recall}).
\end{itemize}

\paragraph{Combined Metrics}
In order to simplify evaluation and provdide a more comprehensive view some metrics combine all of the above. 

\begin{itemize}
	\item \textbf{G-Mean (Geometric Mean):} It balances the ability of the model both on positive and negative cases by taking the square root of the product between sensitivity and specificity.
	\begin{equation}
		\text{G-Mean} = \sqrt{\text{Sensitivity} \times \text{Specificity}}
		\label{eq:gmean}
	\end{equation}
	
	\item \textbf{F-Measure (F1-Score):} It computes the harmonic mean between recall and precision.
	\begin{equation}
		\text{F-Measure} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
		\label{eq:f1score}
	\end{equation}
\end{itemize}

\subsection{Ranking Methods and Metrics}
In cases where class distribution varies such as imbalanced data or the error costs for making a wrong decision can vary , ranking methods are used to provide an overview of performance across all decision thresholds. 

\paragraph{ROC Analysis}
Plots True Positive Rate vs. False Positive Rate across thresholds. A curve near the top-left corner indicates a strong classifier. Particularly effective for imbalanced data.

\paragraph{AUC}
Area Under the ROC Curve; represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative one.

Area Under the ROC Curve , is a metric for how well a classifier distinguished between classes.It actually quantifies the probability that a model assigns a higher probability to a randomly selected positive sample than a negative one . 

\paragraph{Precision--Recall Curves}
Show precision--recall trade-offs across thresholds and are preferred for highly imbalanced datasets.

\medskip
In conclusion, evaluating classifiers on imbalanced datasets requires combining threshold metrics with ranking-based metrics. While threshold metrics evaluate specific operational conditions, ranking methods such as ROC and PR curves provide a more holistic understanding of classifier performance.

\section{Limited Datasets}
The lack of large collections of labeled data is a major challenge in medical machine learning, as large annotated datasets require domain expertise and are both time consuming and and expensive to produce \cite{yaoMachineLearningLimited2021, merkinMachineLearningArtificial2022}. An additional challenge is domain shift - when the distribution of test data differs from the training data , leading to degraded model performance \cite{martinInterpretableMachineLearning2023}. 

Data scarcity and dataset diversity remain the primary barriers for application in dementia reseearch. Recent studies reinforce the hypothesis that limited data can impact model generazibility which in turn hurts clinical adoption and utility in dementia prediciton. \cite{huangEstablishingMachineLearning2024, wangUnderstanding2024}. 


\subsection{Context and Challenges}
 All subsequent tasks such as early diagnosis , disease progression, and tracking are impacted heavily by the lack of data. The lack of large scale, heterogeneous datasets  (neuroimaing , biomarkers , clinical records ), has become the main bottleneck for accurate and general ML models.

\section{Small Sample Sizes}
Even though the application of DL on imaging challenges , especially combined with large image datasets can provide major opportunities , barriers such as data availability and logistical constraints persist \cite{Challenges2022} . Models often lack generalizability , lack of gold standards and they fail to address privacy concerns as well as safety \cite{aljuhaniUse2024}. Studies point to the lack of efficient methodologies to define an appropriate data sample \cite{SampleSizeDeterminationMethodologies2019}. Additionally smaller studies often report inflated accuracy metrics indicating that numbers are upper limits or even superficial at times , meaning there exists systematic bias in the literature \cite{luSmallStudyEffects2022}. 


\section{Image Quality and Variability}
MRI lacks arbitrary units. As a consequence scans become non-comparable across sites. Even after normalization , images contain technical artifacts and specific scanner related effects \cite{benzingerRegionalVariabilityImaging2013}. For volumetric measurements the difference in variance can be up to 10x more depending on the scanner, protocol, and acquisition conditions \cite{kruggelImpactScannerHardware2010}. More quantified metrics are reported in \cite{monteroIntraInterscannerCT2025, bhosekarReviewDeepLearningbased} for scanner maker, model and protocol variability that limit the abililty of CNN networks to generalize. 

\section{Interpretability in Medical Applications}
Black-box models may be able to achieve high accuracy but they face regulatory scrutiny and difficulties in approval for clinical use. More interpretable methods like kernel-based or prototype-based models offer insights that cliniciasn can understand and rely upon for further decision making. On the same basis privacy-preserving, uncertainty estimation and fair techniques such as federated learning or diffusion based data generation can align more with the standards of the medical domain and issues that are most important \cite{begumbektasMachineLearningMedicine2025} . Explainable AI is a term for explainable systems that improve transparency and support ethical standards , but integrating them into the clinical flow workflows and meeting regulatory standards still remains difficult \cite{Survey2024} . ML has made progress and offered advancements in multiple medical domain areas but there are still concerns about bias , privacy and legal accountabilty \cite{MachineLearningHealthcare}. 

\section{Computational Costs}
Deep Learning progress is strongly correlated with rising compute demands , which are quicly becoming economically and envrionmentally unsustainable \cite{thompsonComputationalLimitsDeep2022}. Deep learning research explores how to compress, optimize and accelearate how the models run on the hardware. Newer techniques like pruning, quantization and knowledge distillation have emerged and can shrink models by more than 100 times with minimal loss in accuracy \cite{PDFEfficientDeep2025} . Compact models are necessary for running models on devices with limited computational capacity \cite{liuLightweightDeepLearning2024} . MobileNet and EfficientNet are two  architectures designed for this purpose, still achieving comparable accuracy despite their small size \cite{nampalleDeepMediXDeepLearningDriven2023}. Transfer learnign can also be used to adress computational costs in the training phase and it can improve results when data is scarce \cite{dashTransferLearningBased2024} . Additionally pruning and knowledge distillation methods are key for reducing the size of larger models \cite{gouKnowledgeDistillationSurvey2021, vaderaMethodsPruningDeep2022, hoeflerSparsityDeepLearning}. Finally there is work that applies these techniques to Alzheimer's MRI classification \cite{liClassificationAlzheimersDisease2022}.

\section{Data Distillation}

Data distillation provides a promising avenue for efficient and secure sharing of medical imaging datasets. Instead of sharing full datasets, distilled representations can preserve modeling performance while reducing data volume. Recent investigations show that a small, representative set of distilled images can achieve near-equivalent model performance, demonstrating potential for scalable clinical collaboration \cite{liDatasetDistillationMedical2025}.
