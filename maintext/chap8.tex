\chapter{Recent Advances And Innovations}

\section{Multi-Modal Integration}

Most of the current research on Alzheimer's Disease (AD) classification relies on single modality data. A modality, as defined in \cite{cuiDeep2023}, is an experience like sound, image, or touch.

Even though the scope of this work is not to review or examine works in data fusion, in order to provide a clearer understanding we should mention the challenges that are provided in one of the recent taxonomies of the field. The main challenges are:

\begin{enumerate}
    \item Representation
    \item Translation
    \item Alignment
    \item Fusion
    \item Co-Learning
\end{enumerate}

One study combined multipled modalities like MRI images, genetic ( single Nucleotide Polymorphisms (SNP)) and clinical test scores into a unified deep learning framework. They used stacked denoising autoencoders to rpocess the clinical and genetic inputs, and a 3D CNN for the imaging data. Their deep learning models outperformed traditional classifiers ( SVM , Decision Trees, Random Forests , KNN) across accuracy, precision , recall and F1 score. Additionaly multimodal fusion consistently beat single-modality approaches and the model was able to identify the hippocampus region - well established with AD literature- as key feature. \cite{venugopalanMultimodalDeepLearning2021}.

A second study used a multimodal recurrent neural network (RNN) to predict progression from MCI to AD. Their inputs were neuroimagin markers from cross-sections, CSF  biomarkesrs , and cognitive scores, all coming from ADNI. The combination of all modalities led to an improved accuracy by approximately 6 percentage points. The authors also suggested that mutltmodal approaches could help identiy which mCI patients would benefit most from clinical trial enrollment \cite{leePredictingAlzheimersDisease2019}.

Another study reported that its deep learning fusion network approach performed better overall in classifying NC, MCI, AD, and nADD across the range of clinical tasks \cite{qiuMultimodalDeepLearning2022}.

From the above evidence, it is well understood that more and more research is being implemented in the field by integrating multiple modalities in order to achieve higher classification accuracy, since it seems likely that multi-modal data are more robust and provide a more holistic view of the disease.

\section{Explainable AI }
The role of explainability has become increasingly important in recent years, as deep learning achieves state-of-the-art results and exceeds previous approaches in domains where decision-making is high-stakes (e.g., healthcare, finance).

There are \textbf{three main pillars} that Explainable AI (XAI) aims to cover:

\begin{enumerate}
    \item \textbf{Trust:} Allowing the end user to trust and understand the fundamental reasoning behind the decision-making.
    \item \textbf{Reducing Bias \& Increasing Fairness:} In models that produce wrong behaviors—either due to the overrepresentation of certain groups in human-curated data or algorithmic mistakes—the goal is to increase fairness and reduce bias.
    \item \textbf{Model Improvement \& Debugging:} Enabling developers to identify errors in the model's logic and improve performance.
\end{enumerate}

\section{Definitions and Taxonomy}
In the field, there is no clear consensus regarding the definitions of certain terms. The two most debated definitions are \textbf{interpretability} and \textbf{explainability}. Even though there is a debate surrounding these terms, we will adopt a practical definition to move forward:

\begin{itemize}
    \item \textbf{Interpretability} is considered the ability to understand the model through its parameters or a simple graph. The model can be easily understood by a human (intrinsic explainability).
    \item \textbf{Explainability} is considered the process of providing a valid explanation for the reasoning the model used to arrive at a specific decision (post-hoc explainability).
\end{itemize}

The \textbf{taxonomy} of techniques can be described as:
\begin{itemize}
    \item \textbf{Local \& Global:} (Scope of the explanation)
    \item \textbf{Perturbation \& Gradient-Based Methods:} (Mathematical approach)
    \item \textbf{Model-Agnostic \& Model-Specific:} (Applicability across different model architectures)
\end{itemize}

For a more concise approach, the most noteworthy techniques are \textbf{LIME} and \textbf{SHAP}. Both are primarily considered local approaches; however, SHAP can also be considered a global technique because it produces a sum of all feature attributions, allowing for a holistic view of the dataset.

\subsection{LIME}
In LIME, the goal is to produce a function $g$ that approximates the output of the neural network $f$. It utilizes a weighting variable that is higher for small perturbations ( samples that are closer to the original input are weighed more while those that are not are weighed less). The goal of the algorithm is to find a simpler and accurate surrogate model. 

\begin{equation}
    \xi(x) = \operatorname*{argmin}_{g \in G} \ \mathcal{L}(f,g,\pi_{x}) + \Omega(g)
\end{equation}

Where $\mathcal{L}$ represents the \textbf{Loss} (fidelity of the surrogate model) and $\Omega$ represents the \textbf{Complexity} of the explanation model.

\subsection{SHAP}
This technique comes from Shapley Values from Game Theory. It works by assigning a score to each feature that reflects how much that feature contributes to the output when combined with all other features. 

\begin{equation}
    g(z') = \phi_0 + \sum_{j=1}^{M} \phi_j \cdot z'_j
\end{equation}

Where $z'$ is the simplified feature (coalition vector) and $\phi$ is the attribution score assigned to that feature. By aggregating the scores across different examples, SHAP can provide a holistic view of the importance of each feature in the network.

\subsection{Other Approaches}
Other memorable approaches include Saliency Maps (which are gradient-based) and Occlusion Sensitivity (which is perturbation-based).

For the final category involving concept-based explanations, a notable method is \textbf{TCAV} (Testing with Concept Activation Vectors).

\section{Sanity Checks}
Even though we can create explanations for a problem, we must be aware of the dangers and understand the capabilities of each approach. For this reason, it is suggested to perform sanity checks to ensure robustness. The sanity checks should include:

\begin{enumerate}
    \item \textbf{Stability:} Models should provide the same (or very similar) explanation for small perturbations to the input, up to a constant $\epsilon$.
    \item \textbf{Robustness:} Models injected with adversarial examples should allow the explanation method to identify why those examples are widely different from the distribution.
    \item \textbf{Determinism:} For the exact same input, models should strictly produce the same explanation.
\end{enumerate}

\section{Related Work}
In this work, the authors tried to create a network by combining a predictor with an explainable tool in order to provide accurate diagnosis while using visualization maps to confirm prediction basis. They built a predictor based on an attention mechanism using multi-scale features to teach a network to predict the correct labels representing the input features. They state that the network shows state-of-the-art accuracy and explainability and is able to define critical areas more clearly and with less noise that matches the neuroscience background literature \cite{yuNovelExplainableNeural2022}.

In another study, researchers used data-level fusion of clinical data, MRI segmentation data, and psychological data. They employed many algorithms with Random Forest being the best and achieving the highest score value. They also used SHAP explanations for further explainability \cite{jahanExplainableAIbasedAlzheimers2023}.

Additionally, in a systematic review on explainable AI in Alzheimer's research, it can be seen that most of the approaches contain post-hoc and model-agnostic approaches. Techniques such as SHAP, LIME, Grad-CAM, and LRP are shown to dominate the field. Also interesting are novel approaches using other modalities (speech \& text), along with the current trade-off between accuracy and explainability in the domain of XAI \cite{viswanExplainable2024}.

\section{Data Augmentation \& Transfer Learning}

Even though Deep Learning has performed tremendously in many computer vision tasks and has become a methodology of choice for analyzing medical images \cite{geertlitjensSurvey2017}, these large networks usually require vast amounts of data in order to avoid overfitting. 

Overfitting refers to the process where a model learns a function $f$ with very high variance, perfectly modelling the training data and not being able to generalize to new instances that do not belong to the prior distribution of the training data. In many applications, such as medical imaging, data is limited due to a multitude of factors, but mainly due to the scarcity of datasets annotated by specialists, which is labor-intensive and has a high cost.

Data Augmentation is a collection of techniques used to increase both the quality and the size of an existing dataset to build better deep learning models. The image augmentation algorithms include the following \cite{shortenSurveyImageData2019} (See Table \ref{tab:augmentation} and Figure \ref{fig:aug_techniques}).

\begin{longtable}{@{}l p{10.5cm}@{}}
    \caption{Image Augmentation Techniques}
    \label{tab:augmentation} \\
    \toprule
    \textbf{Technique} & \textbf{Description} \\
    \midrule
    \endfirsthead

    \caption[]{Image Augmentation Techniques (continued)} \\
    \toprule
    \textbf{Technique} & \textbf{Description} \\
    \midrule
    \endhead

    \bottomrule
    \endfoot

    \bottomrule
    \endlastfoot

    \textbf{1. Geometric transformations} & This category includes basic operations like flipping , cropping , rotation, and translation. They are quite easy to apply and are especially beneficial in reducing positional biases. However transformation of this kind can change the labelr (e.g rotating a 6 can produce a 9 ) so care is needed on examples where the labels can change. \\
    \textbf{2. Color space augmentations} & These transformations change pixel values in their respective color channels or histograms. Examples are changing the brightness or contrast to help the models handle different lighting conditions. \\
    \textbf{3. Kernel filters} & This method works by sliding a kernel filter ( $ n\chi n$  matrix).over an image to either blur or sharpen it . Blurring can help with motion blur resistance while sharpening can highlight details. A variant called Patchshuffle Regularization , shuffles pixel values within a window to encourage more robust learning. \\
    \textbf{4. Mixing images} &  This approach combines mutliple images to create new samples. Techniques can eithe be average between two images ( SamplePairing) or they can be concatenating random crops. Although the images can look unnatural , the method can reduce error rates. \\
    \textbf{5. Random erasing} &  Taking inspiration by dropout this method just masks an $n \times m$  patch , with zeros , maximum values, or noise. By hiding different parts it forces the model to look at the entire image , which can help with occlusion problems. \\
    \textbf{6. Feature space augmentation} & Instead of manipulating input images, this technique operates on the lower-dimensional vector representations (feature space) found in high-level network layers. Techniques include adding noise to these vectors or performing interpolations between nearest neighbors (similar to SMOTE) to generate new instances. \\
    \textbf{7. Adversarial training} & This involves using a rival framework to generate "adversarial attacks"—constrained noise injections that cause misclassifications. Using these adversarial examples during training acts as a search algorithm for augmentations, strengthening weak decision boundaries and improving model robustness. \\
    \textbf{8. GANs} & Generative Adversarial Networks (GANs) use a "generator" network to create artificial images and a "discriminator" network to distinguish real from fake ones. This powerful oversampling technique can "unlock" additional information from a dataset and create synthetic training data to increase size and diversity. \\
    \textbf{9. Neural Style Transfer} & This algorithm manipulates sequential representations in a CNN to transfer the artistic style or texture of one image to another while preserving content. It is particularly useful for randomizing environments (e.g., lighting and texture) when transferring models from simulations to the real world. \\
    \textbf{10. Meta-learning} & This refers to using neural networks to optimize other neural networks, specifically for finding optimal augmentation strategies. Examples include "Neural Augmentation" (learning style transfer weights), "Smart Augmentation" (merging images via a network), and "AutoAugment" (using Reinforcement Learning to find optimal transformation policies). \\
    \end{longtable}

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=0.8\textwidth]{Pasted image 20251130003519.png} % Uncomment and fix path
    \caption{Showcases all the available image augmentation techniques. Courtesy of \cite{shortenSurveyImageData2019}.}
    \label{fig:aug_techniques}
\end{figure}

Furthermore, distinct approaches have been developed to generate synthetic training data; one study utilized Deep Convolutional Generative Adversarial Networks (DCGANs) to synthesize Positron Emission Tomography (PET) images across three disease stages, effectively overcoming the lack of labeled data \cite{sajjadDeepConvolutionalGenerative2021}. Similarly, other research employed data augmentation within a transfer learning framework to rectify severe class imbalance in 3D Magnetic Resonance Imaging (MRI) datasets from the OASIS dataset \cite{afzalDataAugmentationbasedFramework2019}.Both studies showed that the methods improved accuracy and diagnosis. 

\subsection{Transfer Learning}

Transfer Learning aims as a technique to improve the performance of a learner in a targeted domain by leveraging knowledge of the learner in other domains that are related. By this method, the data of the target domain can be considerably decreased. In medical imaging, whereas it has been mentioned that data scarcity remains an issue, transfer learning seems promising to overcome this obstacle and produce state-of-the-art transfer learners and leverage domains that are similar with more data to construct better networks.

The different categorizations of transfer learning can be seen in Figure \ref{fig:transfer_learning}, provided by \cite{zhuangComprehensiveSurveyTransfer2020}.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=0.8\textwidth]{Pasted image 20251130135751.png} % Uncomment and fix path
    \caption{Categorizations of transfer learning \cite{zhuangComprehensiveSurveyTransfer2020}.}
    \label{fig:transfer_learning}
\end{figure}

Several approaches in Alzheimer's research leverage transfer learning to detect and classify Alzheimer's disease.

In one study, transfer learning was used to classify four stages of Alzheimer's Disease: Mild Demented, Moderate Demented, Non-Demented, and Very Mild Demented. The deep learning model made use of brain MRI scans and achieved an accuracy of 91.7\%, outperforming previous approaches. More specifically, they used a modified AlexNet architecture which was trained on ImageNet datasets as a source domain to perform knowledge transfer \cite{m.ghazalAlzheimerDiseaseDetection2022}.

Another study used a VGG architecture with already pre-trained weights. They used the foundational network to perform transfer learning and optimized the network using additional MRI images. They show through experiments that with a size almost 10 times smaller on the OASIS MRI dataset, they can perform comparable to or better than some current deep learning approaches \cite{honAlzheimersDiseaseClassification2017}.

