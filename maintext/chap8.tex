\chapter{Recent Advances And Innovations}

\section{Multi-Modal Integration}

Most of the current research on Alzheimer's Disease (AD) classification relies on single modality data. A modality, as defined in \cite{missing_definition}, is an experience like sound, image, or touch.

Even though the scope of this work is not to review or examine works in data fusion, in order to provide a clearer understanding we should mention the challenges that are provided in one of the recent taxonomies of the field. The main challenges are:

\begin{enumerate}
    \item Representation
    \item Translation
    \item Alignment
    \item Fusion
    \item Co-Learning
\end{enumerate}

In one study, they integrated modalities from MRI, genetic (Single Nucleotide Polymorphisms (SNP)), and clinical test data to build a deep learning network by fusing these. The network was composed of stacked denoising auto-encoders to extract features from clinical and genetic data, and a 3D Convolutional Neural Network (CNN) for imaging data. Using data from the ADNI dataset, they demonstrated that deep models outperformed shallow models (SVM, Decision Trees, Random Forests, KNN). In addition, they demonstrated that multi-modality outperforms single modality in metrics such as accuracy, precision, recall, and F1 score. The models also identified the hippocampus and other brain areas as distinguishing features, consistent with AD literature \cite{venugopalanMultimodalDeepLearning2021a}.

Another study tried to predict MCI from AD using a deep learning approach of a multimodal recurrent neural network. The modalities were composed of cross-sectional neuroimaging biomarkers, longitudinal cerebrospinal fluid biomarkers, and cognitive performance biomarkers also obtained by the ADNI dataset. The results showed a significant performance boost of about 6\% in accuracy ($75\% \to 81\%$), while the authors also note that multi-modal approaches seem promising in predicting the MCI stage regarding who will benefit the most from a clinical trial or as a stratification approach within clinical trials \cite{leePredictingAlzheimersDisease2019}.

Another study reported that its deep learning fusion network approach performed better overall in classifying NC, MCI, AD, and nADD across the range of clinical tasks \cite{qiuMultimodalDeepLearning2022}.

From the above evidence, it is well understood that more and more research is being implemented in the field by integrating multiple modalities in order to achieve higher classification accuracy, since it seems likely that multi-modal data are more robust and provide a more holistic view of the disease.

\section{Explainable AI }
The role of explainability has become increasingly important in recent years, as deep learning achieves state-of-the-art results and exceeds previous approaches in domains where decision-making is high-stakes (e.g., healthcare, finance).

There are \textbf{three main pillars} that Explainable AI (XAI) aims to cover:

\begin{enumerate}
    \item \textbf{Trust:} Allowing the end user to trust and understand the fundamental reasoning behind the decision-making.
    \item \textbf{Reducing Bias \& Increasing Fairness:} In models that produce wrong behaviors—either due to the overrepresentation of certain groups in human-curated data or algorithmic mistakes—the goal is to increase fairness and reduce bias.
    \item \textbf{Model Improvement \& Debugging:} Enabling developers to identify errors in the model's logic and improve performance.
\end{enumerate}

\section{Definitions and Taxonomy}
In the field, there is no clear consensus regarding the definitions of certain terms. The two most debated definitions are \textbf{interpretability} and \textbf{explainability}. Even though there is a debate surrounding these terms, we will adopt a practical definition to move forward:

\begin{itemize}
    \item \textbf{Interpretability} is considered the ability to understand the model through its parameters or a simple graph. The model can be easily understood by a human (intrinsic explainability).
    \item \textbf{Explainability} is considered the process of providing a valid explanation for the reasoning the model used to arrive at a specific decision (post-hoc explainability).
\end{itemize}

The \textbf{taxonomy} of techniques can be described as:
\begin{itemize}
    \item \textbf{Local \& Global:} (Scope of the explanation)
    \item \textbf{Perturbation \& Gradient-Based Methods:} (Mathematical approach)
    \item \textbf{Model-Agnostic \& Model-Specific:} (Applicability across different model architectures)
\end{itemize}

For a more concise approach, the most noteworthy techniques are \textbf{LIME} and \textbf{SHAP}. Both are primarily considered local approaches; however, SHAP can also be considered a global technique because it produces a sum of all feature attributions, allowing for a holistic view of the dataset.

\subsection{LIME}
In LIME, the goal is to produce a function $g$ that approximates the output of the neural network $f$. It utilizes a weighting variable that is higher for small perturbations (where we want high fidelity to the original input) and lower for distant, random samples. The algorithm aims to minimize both the error of the surrogate model and its complexity. Therefore, we define the explanation as the argument that minimizes the following objective:

\begin{equation}
    \xi(x) = \operatorname*{argmin}_{g \in G} \ \mathcal{L}(f,g,\pi_{x}) + \Omega(g)
    \label{eq:lime}
\end{equation}

Where $\mathcal{L}$ represents the \textbf{Loss} (fidelity of the surrogate model) and $\Omega$ represents the \textbf{Complexity} of the explanation model.

\subsection{SHAP}
This technique borrows characteristics from Game Theory (specifically Shapley Values). It attempts to assign a score to each feature based on how much it "cooperates" with other features to change the output. The mathematical formulation for the explanation model $g$ is described as:

\begin{equation}
    g(z') = \phi_0 + \sum_{j=1}^{M} \phi_j \cdot z'_j
    \label{eq:shap}
\end{equation}

Where $z'$ is the simplified feature (coalition vector) and $\phi$ is the attribution score assigned to that feature. By aggregating the scores across different examples, SHAP can provide a holistic view of the importance of each feature in the network.

\subsection{Other Approaches}
Other memorable approaches include Saliency Maps (which are gradient-based) and Occlusion Sensitivity (which is perturbation-based).

For the final category involving concept-based explanations, a notable method is \textbf{TCAV} (Testing with Concept Activation Vectors).

\section{Sanity Checks}
Even though we can create explanations for a problem, we must be aware of the dangers and understand the capabilities of each approach. For this reason, it is suggested to perform sanity checks to ensure robustness. The sanity checks should include:

\begin{enumerate}
    \item \textbf{Stability:} Models should provide the same (or very similar) explanation for small perturbations to the input, up to a constant $\epsilon$.
    \item \textbf{Robustness:} Models injected with adversarial examples should allow the explanation method to identify why those examples are widely different from the distribution.
    \item \textbf{Determinism:} For the exact same input, models should strictly produce the same explanation.
\end{enumerate}

\section{Related Work}
In this work, the authors tried to create a network by combining a predictor with an explainable tool in order to provide accurate diagnosis while using visualization maps to confirm prediction basis. They built a predictor based on an attention mechanism using multi-scale features to teach a network to predict the correct labels representing the input features. They state that the network shows state-of-the-art accuracy and explainability and is able to define critical areas more clearly and with less noise that matches the neuroscience background literature \cite{yuNovelExplainableNeural2022}.

In another study, researchers used data-level fusion of clinical data, MRI segmentation data, and psychological data. They employed many algorithms with Random Forest being the best and achieving the highest score value. They also used SHAP explanations for further explainability \cite{jahanExplainableAIbasedAlzheimers2023}.

Additionally, in a systematic review on explainable AI in Alzheimer's research, it can be seen that most of the approaches contain post-hoc and model-agnostic approaches. Techniques such as SHAP, LIME, Grad-CAM, and LRP are shown to dominate the field. Also interesting are novel approaches using other modalities (speech \& text), along with the current trade-off between accuracy and explainability in the domain of XAI \cite{viswanExplainableArtificialIntelligence2024}.

\section{Data Augmentation \& Transfer Learning}

Even though Deep Learning has performed tremendously in many computer vision tasks and has become a methodology of choice for analyzing medical images \cite{litjensSurveyDeepLearning2017}, these large networks usually require vast amounts of data in order to avoid overfitting. 

Overfitting refers to the process where a model learns a function $f$ with very high variance, perfectly modelling the training data and not being able to generalize to new instances that do not belong to the prior distribution of the training data. In many applications, such as medical imaging, data is limited due to a multitude of factors, but mainly due to the scarcity of datasets annotated by specialists, which is labor-intensive and has a high cost.

Data Augmentation is a collection of techniques used to increase both the quality and the size of an existing dataset to build better deep learning models. The image augmentation algorithms include the following \cite{shortenSurveyImageData2019} (See Table \ref{tab:augmentation} and Figure \ref{fig:aug_techniques}).

\begin{longtable}{@{}l p{10.5cm}@{}}
    \caption{Image Augmentation Techniques}
    \label{tab:augmentation} \\
    \toprule
    \textbf{Technique} & \textbf{Description} \\
    \midrule
    \endfirsthead

    \caption[]{Image Augmentation Techniques (continued)} \\
    \toprule
    \textbf{Technique} & \textbf{Description} \\
    \midrule
    \endhead

    \bottomrule
    \endfoot

    \bottomrule
    \endlastfoot

    \textbf{1. Geometric transformations} & This category encompasses simple transformations such as flipping, cropping, rotation, and translation. These methods are generally easy to implement and effectively address positional biases in the training data. However, practitioners must ensure transformations are "safe" and preserve the label (e.g., rotating a '6' might turn it into a '9'). \\
    \textbf{2. Color space augmentations} & Also called photometric transformations, these techniques manipulate pixel values in the RGB color channels or image histograms. Common methods include adjusting brightness, contrast, or isolating color channels to help models overcome lighting biases present in testing data. \\
    \textbf{3. Kernel filters} & This technique involves sliding an $n \times n$ matrix across an image to sharpen or blur it. Blurring can improve resistance to motion blur, while sharpening emphasizes details. A specific variation called PatchShuffle Regularization randomly swaps pixel values within a window to improve robust feature learning. \\
    \textbf{4. Mixing images} & This counterintuitive approach mixes multiple images to create new training instances. Methods include SamplePairing (averaging pixel values of two images) or concatenating random crops. Despite producing unnatural-looking images, this method has proven effective at reducing error rates. \\
    \textbf{5. Random erasing} & Inspired by dropout regularization, this technique randomly selects an $n \times m$ patch of an image and masks it with 0s, 255s, or random values. By removing parts of the input, it forces the model to pay attention to the entire image rather than just a subset, helping address occlusion challenges. \\
    \textbf{6. Feature space augmentation} & Instead of manipulating input images, this technique operates on the lower-dimensional vector representations (feature space) found in high-level network layers. Techniques include adding noise to these vectors or performing interpolations between nearest neighbors (similar to SMOTE) to generate new instances. \\
    \textbf{7. Adversarial training} & This involves using a rival framework to generate "adversarial attacks"—constrained noise injections that cause misclassifications. Using these adversarial examples during training acts as a search algorithm for augmentations, strengthening weak decision boundaries and improving model robustness. \\
    \textbf{8. GANs} & Generative Adversarial Networks (GANs) use a "generator" network to create artificial images and a "discriminator" network to distinguish real from fake ones. This powerful oversampling technique can "unlock" additional information from a dataset and create synthetic training data to increase size and diversity. \\
    \textbf{9. Neural Style Transfer} & This algorithm manipulates sequential representations in a CNN to transfer the artistic style or texture of one image to another while preserving content. It is particularly useful for randomizing environments (e.g., lighting and texture) when transferring models from simulations to the real world. \\
    \textbf{10. Meta-learning} & This refers to using neural networks to optimize other neural networks, specifically for finding optimal augmentation strategies. Examples include "Neural Augmentation" (learning style transfer weights), "Smart Augmentation" (merging images via a network), and "AutoAugment" (using Reinforcement Learning to find optimal transformation policies). \\
    \end{longtable}

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=0.8\textwidth]{Pasted image 20251130003519.png} % Uncomment and fix path
    \caption{Showcases all the available image augmentation techniques. Courtesy of \cite{shortenSurveyImageData2019}.}
    \label{fig:aug_techniques}
\end{figure}

Furthermore, distinct approaches have been developed to generate synthetic training data; one study utilized Deep Convolutional Generative Adversarial Networks (DCGANs) to synthesize Positron Emission Tomography (PET) images across three disease stages, effectively overcoming the lack of labeled data \cite{sajjadDeepConvolutionalGenerative2021}. Similarly, other research employed data augmentation within a transfer learning framework to rectify severe class imbalance in 3D Magnetic Resonance Imaging (MRI) datasets from the OASIS dataset \cite{afzalDataAugmentationbasedFramework2019}. In both instances, the application of these augmentation strategies resulted in significant improvements in model accuracy and diagnostic performance.

\subsection{Transfer Learning}

Transfer Learning aims as a technique to improve the performance of a learner in a targeted domain by leveraging knowledge of the learner in other domains that are related. By this method, the data of the target domain can be considerably decreased. In medical imaging, whereas it has been mentioned that data scarcity remains an issue, transfer learning seems promising to overcome this obstacle and produce state-of-the-art transfer learners and leverage domains that are similar with more data to construct better networks.

The different categorizations of transfer learning can be seen in Figure \ref{fig:transfer_learning}, provided by \cite{zhuangComprehensiveSurveyTransfer2020}.

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=0.8\textwidth]{Pasted image 20251130135751.png} % Uncomment and fix path
    \caption{Categorizations of transfer learning \cite{zhuangComprehensiveSurveyTransfer2020}.}
    \label{fig:transfer_learning}
\end{figure}

Several approaches in Alzheimer's research leverage transfer learning to detect and classify Alzheimer's disease.

In one study, transfer learning was used to classify four stages of Alzheimer's Disease: Mild Demented, Moderate Demented, Non-Demented, and Very Mild Demented. The deep learning model made use of brain MRI scans and achieved an accuracy of 91.7\%, outperforming previous approaches. More specifically, they used a modified AlexNet architecture which was trained on ImageNet datasets as a source domain to perform knowledge transfer \cite{m.ghazalAlzheimerDiseaseDetection2022}.

Another study used a VGG architecture with already pre-trained weights. They used the foundational network to perform transfer learning and optimized the network using additional MRI images. They show through experiments that with a size almost 10 times smaller on the OASIS MRI dataset, they can perform comparable to or better than some current deep learning approaches \cite{honAlzheimersDiseaseClassification2017}.

