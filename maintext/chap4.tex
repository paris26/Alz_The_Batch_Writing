 \chapter{Pre-Processing and Feature Extraction Techniques}

\section{Intensity Normalization}

\subsection{Introduction}
Many processing steps rely on the foundational quality of the underlying image and the standardization of them across a dataset. Techniques such as registration, segmentation, and comparison or classification can be impacted significanlty by variability in the intensity scale of the underlying image. Variability is inherent in MRI images , as scanner-related artifacts , different types of noise , acquisition protocols , and other parameters. \cite{goceriFullyAutomatedAdaptive2018}

To eliminate these issues and provide a standardized quality and scale across all images intensity normalization is employed as a critical pre-processing step. The main goal is to create a standardized scale that accurately represents the underlying tissue, by unifying the mean and variance withiin an image. The performance of this step directly influences the performance of processes responsible for tracking longitudinal disease progression.  \cite{goceriFullyAutomatedAdaptive2018, sunHistogrambasedNormalizationTechnique2015, shahEvaluatingIntensityNormalization2011}.

The goal is to standardize the scale so that the same tissue type will not be depicted in different values because of scanner-related differences and the same tissue should be depicted with the same intensity values across all images. \cite{bagciRoleIntensityStandardization2010, shinoharaStatisticalNormalizationTechniques2014} . Overall this process is essential for downstream algorithms and reliability of further processing. \cite{nyulNewVariantsMethod2000, nyulMethodStandardizingMR2003}

\subsection{Necessity and Significance}
The lack of tissue-specific intensity standardization for MR images creates variability, resulting in differences between two scans of the same subject, even when performed on the same scanner with the same pulse sequence \cite{goceriFullyAutomatedAdaptive2018}. As noted previously, this reduces the accuracy of subsequent processing algorithms. Key factors identified in the literature include:

Without a standardized intensity scale for specific tissue differences can occur even between two scans of the same subject , performed on the same scanner with the same pulse sequence \cite{goceriFullyAutomatedAdaptive2018} . As was also noted previously , this reduces the accuracy of any subsequent algorithm . Several contributing factors are identified in the literature : 

\begin{itemize}
	\item  \textbf{Scanner Related Artifacts} Noise and scanner related artifacts interfere with subsequent processing tasks and must be addressed 
	\item \textbf{Low Contrast and Partial Volume Effects} Different brain tissues should be ideally distinguishable - white matter (WM) , gray matter (GM) , cerebrospinal fluid (CSF) - . In practice , inside a voxel when two types of tissue exist boundaries are usually blurred  \cite{shahEvaluatingIntensityNormalization2011} .
	\item \textbf{Intensity Variability} Differences in scanner hardware, acquisition protocols and data source can also create variations \cite{salomeMRIntensityNormalization2023}. Those variations can occur either between different subjects or withing subsequent scans of the ame patient \cite{kociolekDoesImageNormalization2020}. 
	\item \textbf{Lack of Tissue-Specific Meaning} Unlike CT , which uses standardized Hounsfield units , MRI intensities lack a physical meaning and so they depend on scanner specific settings. This limitation significantly affects longitudinal and quantitative analyses \cite{nyulMethodStandardizingMR2003}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/IntensityNormalization1.png} % Replace with actual filename
    \caption{Brain image depicting artifacts present in MR images and different tissue types. Adapted from \cite{goceriFullyAutomatedAdaptive2018}.}
    \label{fig:brain_artifacts}
\end{figure}

\subsection{Goals and Principles}
The normalization protocol follows the SPIN principles from Shinohara et. al \cite{shinoharaStatisticalNormalizationTechniques2014}. Instead of using a reference histogram as \cite{nyulNewVariantsMethod2000} proposed, this method normalizes each scan based on the white matter intensities and characteristics of its own. The main goals are: 

 The primary goals are:
\begin{enumerate}
    \item \textbf{Consistent measurements} across different scanners
    \item \textbf{Reduced Variability} between subjects and scanning sessions 
    \item \textbf{Meaningful units} that relate to actual biology 
    \item \textbf{Reliable Peformance} even with lesions or imaging artifacts
\end{enumerate}


\subsection{Categories of Intensity Normalization Methods}
Methods can be broadly categorized into:
\begin{enumerate}
    \item Classical/Statistical Methods
    \item Feature-Based Harmonization
    \item Deep Learning-Based Approaches
\end{enumerate}

\subsubsection{Classical Methods}

\paragraph{1. Min-Max Normalization}
This technique tries to squeeze all data values into a standard range, usually betweeen 0 and 1 . The smallest value becomes zero and the largest value becomes one, and everything else gets proportionately scaled. \cite{pandeyComparativeAnalysisKNN2017, patroNormalizationPreprocessingStage2015}

\begin{equation}
    v' = \frac{v - \min(A)}{\max(A) - \min(A)}
    \label{eq:minmax}
\end{equation}
Where $v'$ is the normalized value, $v$ is the original data point, and $\min(A)$ and $\max(A)$ are the minimum and maximum feature values, respectively.

\paragraph{2. Z-Score Normalization}
This method adjusts each intensity value by subtracting from the voxel the average intensity value and dividing by the standard deviation (without background). The result is a value that showcases how far it is from the average, measured in standard deviation units \cite{salomeMRIntensityNormalization2023} . An important drawback of the technique can be that bright regions may be compressed towards the average , potentially losing information. 

\begin{align}
    \mu &= \frac{1}{|B|} \sum_{b \in B} I(b) \\
    \sigma &= \sqrt{\frac{\sum_{b \in B} (I(b) - \mu)^2}{|B| - 1}} \\
    I_{\text{norm}}(x) &= \frac{I(x) - \mu}{\sigma}
    \label{eq:zscore}
\end{align}

\paragraph{3. Mean and Standard Deviation Setting ($\mu \pm 3\sigma$ / M-std)}
This technique adjusts images in a certain way in order to fix their statistical value, typically set the mean to $\mu$ and the standard deviation $\sigma$ to 1 \cite{collewetInfluenceMRIAcquisition2004, albertComparisonImageNormalization2023}. Some implementations confine intensity values within 3 standard deviations of the mean $\mu \pm \sigma$, This can work really well for pictures that follow a normal distribution $N(\mu , \sigma)$ , but images that do not follow one may lose information due to value clippping \cite{kociolekDoesImageNormalization2020}. 

\paragraph{4. Percentile Method}
This technique uses specific percentiles ( usually the 5th and 95th percentiles ) to define the range of the intensity values , which removes all the edge outliers \cite{albertComparisonImageNormalization2023}. 

\paragraph{5. Histogram Matching}
One of the most widely-used approaches , which works by trying to match the image to a reference histogram \cite{nyulNewVariantsMethod2000}

\begin{itemize}
	\item \textbf{Nyul and Udupa's Method:} At first the statistical landmarks of the training images are learnd to create a standard distribution. Then , new images are transformed to match the standard using piecewise linear mapping. 
	\item \textbf{Joint Histogram Matching:} Take pixel pairs and considers the relation of the input to the reference image. While it can potentially be more accurate it requires more computation cite{sunHistogrambasedNormalizationTechnique2015}.  
\end{itemize}

\paragraph{6. Homomorphic Filtering}
This technique tries to adress uneven lighting that exists in images by separating the tissue from the illumination. The intensity of an image $I(x, y)$ can be represented as 
\begin{equation}
I(x,y) = L(x,y) \cdot R(x,y)
\end{equation}
where $L$ represents illumination ( ligthing variations) and $R$ reprents the tissue reflectance. 
Taking the logarithm turns multiplication into addition:
\begin{equation}
	\ln(I) = \ln(L) + \ln(R) 
\end{equation}
Turning the image into the frequency domain help remove slow-varying illumination ( low frequencies ) and keep tissue boundaries and details ( high frequencies )  \cite{goceriFullyAutomatedAdaptive2018}

\paragraph{7. Gaussian Filtering}
Used for smoothing, this requires careful selection of the $\sigma$ parameter. A small $\sigma$ may be inefficient for normalization, while a large $\sigma$ can cause loss of edge information.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/IntensityNormalization2.png}
    \caption{Original image (a); Image obtained by Gaussian filtering with increasing $\sigma$ values (b-d).}
    \label{fig:gaussian_filtering}
\end{figure}

\paragraph{8. White Stripe Normalization}
Proposed by Shinohara et al. \cite{shinoharaStatisticalNormalizationTechniques2014}, this method normalizes based on Normal-Appearing White Matter (NAWM). It identifies the NAWM distribution ("white stripe") and matches its moments (mean and standard deviation) across subjects, providing biologically interpretable units robust to pathology.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/IntensityNormalization3.png}
    \caption{White Stripe Normalization concept.}
    \label{fig:white_stripe}
\end{figure}

\paragraph{9. Fuzzy C-Means (FCM) and Kernel Density Estimation (KDE)}
Fuzzy clustering assigns data points a membership function (0 to 1) indicating proximity to a cluster center. Normalization uses tissue segmentation to scale intensity relative to tissue means.

\textit{Single-Tissue Normalization:} Normalize voxel $x$ by the mean intensity of tissue $T$:
\begin{equation}
    I_{\text{norm}}(x) = \frac{I(x)}{\mu_T}
\end{equation}

\textit{Two-Tissue (Min-Max) Normalization:}
Using two tissue means $\mu_1$ and $\mu_2$, define $a = \min(\mu_1, \mu_2)$ and $b = \max(\mu_1, \mu_2)$. The image is rescaled via:
\begin{equation}
    I_{\text{norm}}(x) = \frac{I(x) - a}{b - a}
\end{equation}

\textit{Kernel Density Estimation (KDE):}
KDE approximates the probability distribution function to smooth the histogram and locate peaks (e.g., white matter peak $\pi$) for normalization:
\begin{equation}
    p(x) = \frac{1}{NMLh} \sum_{i=1}^{NML} K\left(\frac{x - x_i}{h}\right)
\end{equation}
The image is then normalized as $I_{\text{norm}}(x) = c \cdot \frac{I(x)}{\pi}$.

\subsubsection{Feature-Based Harmonization}
\paragraph{ComBat}
Originally for gene expression, ComBat is a feature-based method applied to imaging to eliminate batch effects \cite{orlhacGuideComBatHarmonization2022}. It assumes feature similarity and allows only for shift or spread in the data.

\subsubsection{Deep Learning Methods}
Deep learning has emerged as a powerful tool for normalization, particularly in multi-site analysis.


\paragraph{1. Autoencoders (AEs) and Generative Adversarial Networks (GANs)}
An autoencoder is an algorithm designed to learn an informative representation of data by reconstructing input observations \cite{michelucciIntroductionAutoencoders2022}. It encodes input $x_i$ into a latent representation $h_i = g(x_i)$ and reconstructs it via $X_i = f(h_i)$. Training minimizes the reconstruction error:
\begin{equation}
    \operatorname*{arg\,min}_{f,g} \Delta(X_i, f(g(x_i)))
\end{equation}

GANs consist of two networks: a Generator that creates data and a Discriminator that evaluates it. In the context of normalization, authors have proposed using an autoencoder as the generator within a GAN framework. By utilizing adversarial training, the model learns to produce images that keep the structural integrity of brain anatomy while removing scanner related artifacts which the discriminator network identifies and flags , which improves its ability to work across different imaging sites \cite{delisleRealisticImageNormalization2021, xuStainNormalizationHistopathological2025}. 


\paragraph{2. Adversarial and Task-Driven Normalization}
This approach uses two networks , one is a normalization network and the second a segmentation network. By combining the adversarial loss function with the loss function of the segmentation network it allows for a pipeline that can outperform generic domain adaptation as the normalization is made in order to maximize the segmentation of the tissue. \cite{delisleRealisticImageNormalization2021}


\paragraph{3. Disentanglement Networks}
Disentaglement networks split the image content code ( shapes , layout, geometry ) with the style code ( colors , textures ) , where the first one is domain independent whereas the second one domain specific. This helps evade the assumption that unsupervised image-to-image translation has that there is a one to one mapping. This allows frameworks like MUNIT to normalize content across different source domains. \cite{xuStainNormalizationHistopathological2025} 

\subsection{Impact of Intensity Normalization}

\paragraph{Image Registration}
Intensity normalization can improve the performance of algorithmic registration because its goal is to ensure that similar tissue will have the same intensity values. This in turn increases the reliability of similarity metrics and leads to better image alignment \cite{sunHistogrambasedNormalizationTechnique2015, bagciRoleIntensityStandardization2010}.

\paragraph{Image Segmentation}
One other subsequent task that benefits with intensity normalization is image segmentation. Since variations in tissue can negatively affect segmentation and volume estimation, normalization greatly enhances performance to reduce the variations within an image, even more so at multi-site datasets \cite{shahEvaluatingIntensityNormalization2011a}.

\paragraph{Texture Classification/Radiomics}
In texture analysis spatial gray-variations , are highly sensitive to acquisition protocols and the technique relies on them. Intensity normalization therefore is essential to ensure meaningful features. However they should be applied carefully , since their effects may depend on the imaging sequence (e.g. T1, T2 , FLAIR)  \cite{kociolekDoesImageNormalization2020}. 

\paragraph{Brain Template Construction}
Normalization can enable the calculation of different brain volume statistics for GM , WM and CSF. Post-Normalization brain templates show clearer tissue separation , making the boundaries between different types of tissue more distinct \cite{sunHistogrambasedNormalizationTechnique2015}.

\subsection{Challenges and Considerations}
Even though its use can be critical for many subsequent tasks there are still challenges in the use of intensity normalization as a pre-processing step. FIrst of all the choice of method is mostly sequence-dependent \cite{salomeMRIntensityNormalization2023}. Additionally deep learning frameworks outperform classical methods only in some specific cases , probably because of data scarcity , since deep learning even though very promisisng is upper limited by the existence of diverse and large quanitty datasets \cite{albertComparisonImageNormalization2023} . Overall the selection of an algorithm and an appropriate strategy remains one of the pivotal steps for an MR pipeline.  

\section{Denoising}

Denoising is an important step in the pre-processing of images with the goal of using them as training data for a learning algorithm. Noise and artifacts can significantly impact the performance of such algorithms, which is why denoising is often included as a standard pre-processing step---especially when the quality of the raw images varies or when data come from multiple scanners (inherent variability).

In denoising, we assume there exists an ``ideal'' clean image represented as a vector $x \in \mathbb{R}^{N}$. However, what we observe is a noisy measurement $y$, which contains noise.

The most common mathematical noise model, due to the Central Limit Theorem and ease of use, is Additive White Gaussian Noise (AWGN). The relationship is:
\[
y = x + v.
\]

In this model, the noise is assumed to be drawn from a Gaussian distribution with mean zero and variance $\sigma^2$:
\[
v \sim \mathcal{N}(0, \sigma^2 I).
\]
This means that each pixel is corrupted by a random value sampled from that distribution.

The goal is to find a function $D$ such that $\hat{x} = D(y, \sigma)$, producing an estimate as close as possible to the original $x$.

Since we know the noisy image and the noise distribution, we can formulate the problem using Bayesian inference. We aim to maximize the posterior probability:
\[
p(x \mid y) = \frac{p(y \mid x) \, p(x)}{p(y)}.
\]

Taking the negative logarithm turns the product into a sum, resulting in the minimization objective:
\[
-\log p(x \mid y) \propto 
\underbrace{-\log p(y \mid x)}_{\text{Data Fidelity}} 
+ 
\underbrace{-\log p(x)}_{\text{Prior}}.
\]

Because of the Gaussian noise model, the likelihood is:
\[
p(y \mid x) \propto 
\exp\left(
    -\frac{\|y - x\|^2}{2\sigma^2}
\right).
\]

Taking the negative log removes the exponential:
\[
-\log p(y \mid x) = \frac{1}{2\sigma^2} \|y - x\|^2.
\]

The second term, $-\log p(x)$, serves as a prior and becomes a regularization term. The denoising objective becomes:
\[
\hat{x} = 
\arg\min_x 
\underbrace{\|y - x\|^2}_{\text{Match the Data}}
+ 
\lambda \cdot 
\underbrace{\rho(x)}_{\text{Be a Real Image}}.
\]

This formulation motivated the field’s ``evolution of priors,'' producing many classical denoising approaches.

Two of the most important classical algorithms are NLM and BM3D.

\subsection{Non-Local Means (NLM)}

NLM \cite{buadesReviewImageDenoising2005} grew from the idea of \emph{spatial smoothness}. Earlier algorithms assumed images should be spatially homogeneous, so averaging pixels in local neighborhoods could remove noise while preserving signal. These methods often produced blurry results, especially near edges.

NLM instead uses neighborhoods (patches) of pixels and the concept of \emph{self-similarity} across the entire image. For each pixel $p$, a patch $P$ surrounding it is compared to patches $Q$ around every other pixel $q$, using a similarity measure such as Euclidean distance $\|P - Q\|^2$.

The denoised estimate averages all pixels, but with weights proportional to patch similarity:
\[
\hat{x}(p) = 
\frac{\sum_q w(p,q)\, y(q)}{\sum_q w(p,q)}.
\]

\subsection{BM3D}

After NLM, the creation of algorithms such as BM3D improved denoising performance so dramatically that researchers questioned whether denoising had reached a theoretical limit.

BM3D stands for \emph{Block-Matching and 3D Filtering}. Like NLM, it finds similar patches, but instead of averaging them directly, it stacks them into a 3D group. A transform (3D wavelet or DCT) is applied so that signal becomes concentrated in a few large coefficients while noise spreads into many small ones. Thresholding the small coefficients and inverting the transform yields high-quality results. \cite{danielyanBM3DFramesVariational2011} 

\subsection{Deep Learning Era}

Before deep learning, humans designed the transforms (e.g., DCT) used to enforce sparsity. With deep learning, the network learns the appropriate transform directly from data.

We define a parametric function $f_\theta$ (a neural network) with parameters $\theta$. The network takes a noisy image $y$ and outputs a clean estimate $x$.

For supervised learning with an MSE loss:
\[
\min_\theta \sum_{i=1}^N 
\| f_\theta(y_i) - x_i \|^2.
\]

Since clean images are not always available, a common strategy is to corrupt clean images with Gaussian noise and train the model to recover them.

No single architecture is universally best; much progress arises from empirical experimentation to identify the best-performing denoiser.

A recently expanding research direction, highlighted in \cite{eladImageDenoisingDeep2023}, uses denoising network priors as generative models capable of synthesizing new images. Using iterative methods such as Langevin dynamics, these priors can generate realistic images from noise. While not the primary focus of this work, such techniques can be beneficial in domains like medical imaging, where data scarcity is a major challenge.



% \section{Skull Stripping}
% 
% High-resolution MRI images contain non-brain tissues such as skin, fat, muscle, neck structures, and eyeballs, unlike other modalities such as PET scans. The presence of these non-brain tissues can severely impact automated processing algorithms such as image segmentation and analysis techniques. Therefore, quantitative morphometric studies of MR brain images—such as those used in Alzheimer's disease research—commonly require a preprocessing step to remove non-brain and extra-cranial tissues \cite{kalavathiMethodsSkullStripping2016}.
% 
% The MRI system produces brain images as 3D volumetric data composed of 2D slices. Further computer-aided processing is essential in order to extract meaningful information, whether for research, diagnostic, or clinical purposes.
% 
% As noted earlier, quantitative morphometric studies require isolating brain tissue from non-brain tissue, a process referred to as \emph{skull stripping}. Automated skull-stripping enhances segmentation accuracy, making manual or automated segmentation methods more reliable. Examples from \cite{kalavathiMethodsSkullStripping2016} are shown in Figure~\ref{fig:skullstrip_example}.
% 
% \begin{figure}[h!]
% \centering
% \includegraphics[width=0.8\linewidth]{images/Skull Stripping image.png}
% \caption{Examples of automated skull stripping from \cite{kalavathiMethodsSkullStripping2016}.}
% \label{fig:skullstrip_example}
% \end{figure}
% 
% Voxel-based morphometry (VBM) results also showed significant improvements after skull stripping was applied \cite{rehmanConventionalDeepLearning2020}. Many brain imaging applications benefit from the precise segmentation of the brain from surrounding tissues.
% 
% Skull-stripping algorithms are typically evaluated by their speed and their impact on downstream automated tasks such as segmentation.
% 
% Multiple skull-stripping techniques have been developed due to their success and effectiveness in improving diagnostic and prognostic accuracy \cite{kalavathiMethodsSkullStripping2016}.
% 
% Manual brain segmentation is considered the most robust and accurate approach, but it is extremely laborious, time-consuming, and subject to inter-clinician variability. Manual masks are often treated as the ground truth against which automated methods are validated \cite{rehmanConventionalDeepLearning2020}. This has led to a strong need for automated skull stripping.
% 
% Despite significant progress, many skull-stripping approaches still perform well only on specific datasets and often require tuning of hyperparameters. According to \cite{rehmanConventionalDeepLearning2020}, skull-stripping methods fall into three primary categories:
% 
% \begin{enumerate}
% \item Manual skull stripping
% \item Classical approaches
% \item Deep learning–based approaches
% \end{enumerate}
% 
% \begin{figure}[h!]
% \centering
% \includegraphics[width=0.8\linewidth]{images/Skull Stripping Techniques.png}
% \caption{Overview of skull-stripping categories \cite{rehmanConventionalDeepLearning2020}.}
% \end{figure}
% 
% \subsection{Skull Stripping Methods}
% 
% Based on \cite{kalavathiMethodsSkullStripping2016}, skull-stripping techniques can be grouped as follows:
% 
% \begin{itemize}
% \item Mathematical morphology-based methods
% \item Intensity-based methods
% \item Deformable surface-based methods
% \item Atlas-based methods
% \item Hybrid methods
% \item Deep learning–based methods
% \end{itemize}
% 
% \subsection{Morphology-Based Methods}
% 
% These methods use morphological erosion and dilation operations—combined with thresholding and edge detection—to estimate the initial region of interest (ROI) and separate brain from non-brain tissues.
% 
% A key drawback is that performance depends heavily on empirically tuned parameters related to the shape and size of the morphological structuring elements.
% \subsubsection{\textit{Examples}}
% \begin{enumerate}
%     \item \textbf{Brain Surface Extraction (BSE).}\\
%     Uses anisotropic diffusion filtering, Marr–Hildreth edge detection, and morphological operations such as erosion and dilation. BSE is fast (=3.5 seconds) \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{Brain Extraction Algorithm.}\\
%     Uses diffusion, morphological operations, and connected component analysis for T1W and T2W MRI \cite{rehmanConventionalDeepLearning2020}.
% 
%     \item \textbf{SMHASS (2012).}\\
%     Combines deformable models with histogram analysis and morphological preprocessing \cite{kalavathiMethodsSkullStripping2016}.
% \end{enumerate}
% 
% 
% \subsection{Intensity-Based Methods}
% 
% These methods classify brain vs.\ non-brain regions using pixel intensities. Examples include histogram-based, region-growing, and edge-based methods. Their primary limitation is sensitivity to MRI noise, low contrast, and bias field artifacts.
% 
% \subsubsection{\textit{Examples}}
% \begin{enumerate}
%     \item \textbf{Graph Cuts (GCUT, 2010).}\\
%     Uses graph-theoretic segmentation to isolate brain tissue from dura, beginning with thresholding followed by connected submask selection \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{Region Growing (RG).}\\
%     Forms connected regions based on local intensity similarity. Variants include 2D approaches for coronal slices and MARGA for axial views and low-quality images \cite{kalavathiMethodsSkullStripping2016}.
% \end{enumerate}
% 
% 
% \subsection{Deformable Surface-Based Methods}
% 
% These methods employ active contours (snakes) that evolve iteratively according to an energy functional. The contour adapts—shrinking or expanding—to match the brain boundary. Level-set formulations provide a robust mathematical framework.
% 
% Their performance depends strongly on the initial contour and image quality, especially the clarity of edges.
% 
% \textit{Examples}
% 
% \begin{enumerate}
%     \item \textbf{BET (Brain Extraction Tool, 2002).}\\
%     Widely used, fast, and freely available. It expands a tessellated sphere to match brain edges using adaptive forces \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{Model-Based Level Set (MLS, 2006).}\\
%     Uses curvature and intensity-derived forces to evolve an active contour \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{3dSkullStrip (AFNI, 2005).}\\
%     BET-like but includes improvements to avoid eyes and ventricles \cite{kalavathiMethodsSkullStripping2016}.
% \end{enumerate}
% 
% 
% \subsection{Atlas / Template-Based Methods}
% 
% These methods register the subject MRI to one or more anatomical atlases, enabling the transfer of brain masks to the subject.
% 
% \subsubsection{\textit{Examples}}
% \begin{enumerate}
%     \item \textbf{MAPS (2011).}\\
%     Combines multiple atlas registrations to produce a consensus segmentation \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{BEaST (2012).}\\
%     Uses nonlocal segmentation with multi-resolution priors \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{Pincram (2015).}\\
%     Employs iterative refinement to propagate labels from multiple atlases \cite{kalavathiMethodsSkullStripping2016}.
% \end{enumerate}
% 
% \subsection{Hybrid Methods}
% 
% Hybrid approaches combine multiple algorithms or features to improve robustness and accuracy.
% 
% \subsubsection{\textit{Examples}}
% 
% \begin{enumerate}
%     \item \textbf{SPECTRE (2011).}\\
%     Integrates elastic registration, tissue segmentation, and morphological watershed operations \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{HWA (Hybrid Watershed Algorithm, 2004).}\\
%     Combines watershed segmentation with deformable surface models \cite{kalavathiMethodsSkullStripping2016}.
% 
%     \item \textbf{BEMA (2004).}\\
%     Runs multiple extractors (BET, BSE, Watershed, etc.) in parallel and merges the results \cite{rehmanConventionalDeepLearning2020}.
% \end{enumerate}
% 
% \subsection{Deep Learning-Based Methods}
% Deep learning approaches include 2D and 3D CNNs. While 3D CNNs capture 3D context, they require much higher computational resources. DL methods are generally categorized as:
% 
% \begin{itemize}
% \item Patch-based CNNs
% \item Encoder–decoder CNNs (e.g., U-Net)
% \end{itemize}
% 
% Encoder–decoder architectures typically perform better, are faster, and can capture global and local features.
% 
% However despite their performance , deep learning mehods showcase several limitations, such as : 
% \begin{itemize}
% \item The requirement of large annotated datasets
% \item The inability of hyperparameter tuning due to the black-box behavior of the networks
% \item The sensitivity that emerges when trained on healthy individuals
% \end{itemize}
% 
% \subsection{Comparative Analysis}
% 
% \subsection{Automated vs.\ Manual Approaches}
% 
% \begin{itemize}
% \item Automated methods are faster but may require parameter tuning.
% \item Semi-automated methods are accurate but slow and user-dependent.
% \end{itemize}
% 
% \subsection{Evaluation Metrics}
% 
% \begin{itemize}
% \item Dice coefficient
% \item Jaccard Index
% \item Sensitivity / Specificity
% \item False Positive Rate / False Negative Rate
% \item Hausdorff Distance
% \item Average Symmetric Surface Distance (ASSD)
% \end{itemize}
% 
% Common metrics include:
% 
% \begin{itemize}
% \item Dice coefficient
% \item Jaccard Index
% \item Sensitivity / Specificity
% \item False Positive Rate / False Negative Rate
% \item Hausdorff Distance
% \item Average Symmetric Surface Distance (ASSD)
% \end{itemize}
% 
% Comparative findings from \cite{kalavathiMethodsSkullStripping2016,rehmanConventionalDeepLearning2020} include:
% 
% \begin{itemize}
% \item McStrip (hybrid) outperforms BET and BSE.
% \item HWA shows high sensitivity and robustness.
% \item Deep learning achieves highest Dice and specificity; 3D U-Net has highest sensitivity.
% \end{itemize}
% 
% \subsection{Strengths and Weaknesses of Skull Stripping Approaches}
% 
% \begin{table}[h!]
% \centering
% \begin{tabularx}{\linewidth}{|X|X|X|}
% \hline
% \textbf{Methodology} & \textbf{Strengths} & \textbf{Weaknesses} \\
% \hline
% Mathematical Morphology & Simple to implement; fast & Parameter-dependent; noise-sensitive; risk of over/under-segmentation \\
% \hline
% Intensity-Based & Uses fundamental image properties & Sensitive to noise, bias field, threshold choice; watershed over-segmentation \\
% \hline
% Deformable Surface & Accurate and robust; boundary-aware & Sensitive to initialization; noise-sensitive; may fail in non-standard cases \\
% \hline
% Atlas-Based & Leverages anatomical priors; good when intensities are unreliable & Dependent on registration quality; computationally intensive \\
% \hline
% Hybrid & Combines strengths of multiple methods; often fully automatic & Complex; inherits weaknesses of contributing methods \\
% \hline
% Deep Learning & State-of-the-art performance; learns features automatically & Requires large datasets; expensive; black-box behavior \\
% \hline
% \end{tabularx}
% \caption{Comparative strengths and weaknesses of skull-stripping methodologies.}
% \end{table}
% 
% \subsection{Primary Challenges}
% 
% Major difficulties arise from:
% 
% \begin{itemize}
% \item MRI artifacts (noise, intensity bias, motion)
% \item Anatomical complexity and overlapping tissue intensities
% \item Presence of pathology (e.g., tumors) affecting segmentation accuracy
% \end{itemize}

\input{maintext/chapter4_fully_cited}


\section{Voxel-Based Morphometry (VBM)}

VBM is a whole-brain statistical technique that replaced the manual procedure in which clinicians had to track a specific brain structure, such as the hippocampus, across sequential slices. This technique frees the user from being confined to a single region of interest and instead enables analysis of the entire brain, detecting even subtle structural changes.

The VBM pipeline consists of several key steps:

\begin{enumerate}
    \item \textbf{Image Acquisition} \\
    This typically involves acquiring a high-resolution T1-weighted MRI scan.

    \item \textbf{Spatial Alignment (Normalization)} \\
    All images must be aligned to a common template to allow voxel-wise statistical comparison.

    \item \textbf{Tissue Segmentation} \\
    Tissues are automatically classified into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF). 
    This relies on spatial priors and voxel intensities. Formally:
    \[
        P(\text{Tissue Class} \mid \text{Intensity}, \text{Location})
    \]

    \item \textbf{Modulation} \\
    Modulation preserves the total amount of tissue after spatial transformations using the Jacobian determinant.  
    The choice to use modulation depends on the research question:

    \begin{itemize}
        \item Without modulation: analysis reflects \emph{concentration} (e.g., GM density per unit volume).
        \item With modulation: analysis reflects \emph{absolute volume}.
    \end{itemize}
    
    Modulation may miss microscopic changes, and it is suggested that modulation should not be used to detect mesoscopic abnormalities such as cortical thinning \cite{raduaValidityModulationOptimal2014}.


    \item  \textbf{Smoothing} \\
    An isotropic Gaussian kernel is applied to:

    \begin{enumerate}
        \item Increase signal-to-noise ratio by averaging neighboring voxels.
        \item Compensate for minor spatial misalignments after normalization.
        \item Render the data more normally distributed for parametric statistics.
    \end{enumerate}

    \item \textbf{Statistical Analysis} \\
    Voxel-wise statistical tests are typically performed using the General Linear Model (GLM).  
    A t-value map (Statistical Parametric Map) is created by comparing group means and accounting for variance.
    
    To control false positives:
    \begin{itemize}
        \item \textbf{Family-Wise Error (FWE)} correction
        \item \textbf{False Discovery Rate (FDR)}
    \end{itemize}

    
\end{enumerate}


\subsection{Evolution of VBM}

VBM initially faced criticism regarding registration and segmentation accuracy. Two major improvements emerged:

\begin{itemize}
    \item \textbf{Optimized VBM} \\
        Introduced study-specific templates and better skull-stripping to improve normalization accuracy.
    \item  \textbf{DARTEL} \\ 
        A diffeomorphic algorithm using exponentiated Lie algebra for high-dimensional registration.

        The algorithm iteratively computes an increasingly accurate group average and stores subject-specific deformations as \emph{individual flow fields}.
\end{itemize}


\subsection{Alzheimer's Disease and Atrophy Patterns}

\begin{enumerate}

    \item \textbf{Medial Temporal Lobe} \\
    The earliest and most critical changes occur here, involving structures essential for episodic memory.

    \item \textbf{Temporoparietal Association Cortex} \\
    Includes the temporal lobe, parietal lobe, and angular gyrus—responsible for memory, spatial cognition, and language processing.

    \item \textbf{Posterior Cingulate Cortex and Precuneus} \\
    Key nodes of the Default Mode Network (DMN), implicated in self-referential thought and autobiographical memory.

    \item \textbf{Frontal Lobe Involvement} \\
    VBM helped demonstrate that the frontal lobe is also affected, which has significant clinical implications for symptoms such as apathy and disinhibition.

    \item \textbf{Unaffected Regions} \\
    The primary visual cortex, primary sensorimotor cortex, and the cerebellum remain largely intact.

    \item \textbf{VBM and Braak Staging} \\
    Braak staging describes the sequential spread of neurofibrillary tangles (NFTs).  
    VBM-detected atrophy patterns mirror those found in post-mortem studies, suggesting VBM can track macroscopic consequences of NFT accumulation.
    
    \item \textbf{Structural Integrity vs. Metabolism} \\
    Hypometabolism may precede structural loss, meaning that VBM cannot always capture early dysfunction \cite{duongHypometabolicMismatchAtrophy2025, chetelatDirectVoxelbasedComparison2008}.

\end{enumerate}

\begin{table}[htbp]
\centering
\small
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}p{2.1cm}}
\hline
\textbf{Brain Region} & \textbf{Functional Role} & \textbf{Atrophy Pattern in AD} & \textbf{Supporting Imaging Reference} & \textbf{Figure Link} \\
\hline
Medial Temporal Lobe (hippocampus, entorhinal cortex) & Episodic memory encoding \& consolidation & \textbf{Earliest and most severe atrophy} on MRI/VBM; hallmark of typical AD progression. & VBM \& MRI studies document significant grey matter loss here. & Fig.~\ref{fig:vbm-mtl} \\
Temporoparietal Cortex (posterior temporal \& parietal lobes, angular gyrus) & Memory integration, spatial cognition, language & \textbf{Substantial atrophy} in typical AD; also key to posterior cortical involvement. & Typical AD shows loss in these regions relative to controls. & Fig.~\ref{fig:vbm-tp} \\
Posterior Cingulate Cortex \& Precuneus & Default Mode Network (DMN), self-referential processing & \textbf{Significant atrophy}, often seen with hypometabolism even before volume loss. & Posterior midline atrophy linked to DMN disruption. & Fig.~\ref{fig:vbm-pcc} \\
Frontal Lobes & Executive function, behavior regulation & \textbf{Later-stage involvement} with more diffuse atrophy; less prominent early. & Atrophy spreads later toward frontal regions in AD. & Fig.~\ref{fig:vbm-frontal} \\
Primary Visual, Sensorimotor Cortex, Cerebellum & Sensory/motor processing & \textbf{Relatively spared} compared to associative and memory networks in early/mild AD. & Primary regions less affected in typical AD. & Fig.~\ref{fig:vbm-primary} \\
Structural vs Metabolic Changes & -- & Hypometabolism can occur before detectable grey matter loss. & FDG PET shows metabolic changes preceding volume loss. & Fig.~\ref{fig:vbm-metabolic} \\
\hline
\end{tabularx}
\caption{Summary of regional atrophy patterns in AD and linked VBM-related figures.}
\label{tab:vbm-atrophy-summary}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/nihms154848f1.jpg}
    \caption{"Results of the VBM comparisons of typical AD, atypical AD and FTLD groups with controls. Results are shown on 3D renderings of the brain and representative coronal slices through the customized template." \cite{whitwellTemporoparietalAtrophyMarker2011}}
    \label{fig:vbm-mtl}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/nihms154848f2.jpg}
    \caption{"Results of the conjunction analyses shown on 3D renderings of the brain. Two conjunction analyses are shown: 1) conjunction of “atypical AD versus controls” and “typical AD versus controls”, and 2) conjunction using the three clinical variants of atypical AD: “aphasic dementia versus controls”, “CBS versus controls”, and “bvFTD versus controls” \cite{whitwellTemporoparietalAtrophyMarker2011}}
    \label{fig:vbm-tp}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/nihms154848f3.jpg}
    \caption{"Results of VBM direct comparisons between the typical and atypical AD groups, and between the atypical AD and FTLD groups. The typical AD and FTLD groups showed greater grey matter loss in the hippocampi than the atypical AD group. Conversely, the atypical AD group showed greater loss in the left putamen than the typical AD group, and greater loss in the temporoparietal cortex than the FTLD group. \cite{whitwellTemporoparietalAtrophyMarker2011}}
    \label{fig:vbm-pcc}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/nihms154848f4.jpg}
    \caption{"Regions of grey matter loss in each of the different atypical clinical diagnosis groups (aphasic dementia, corticobasal syndrome and behavioral variant frontotemporal dementia) when compared to controls." \cite{whitwellTemporoparietalAtrophyMarker2011}}
    \label{fig:vbm-frontal}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/nihms-137059-f0004.jpg}
    \caption{"Patterns of grey matter loss identified in the aMCI-P group compared to controls (corrected for multiple comparisons, p<0.05). The patterns of cortical atrophy are shown on a 3D surface render (top). In addition the results are shown on a sagittal and coronal slice through the customized template, selected to highlight changes in the cingulate cortex and the medial temporal lobes (bottom). L = left; R = right" \cite{whitwellMRIPatternsAtrophy2008}}
    \label{fig:vbm-primary}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/nihms-137059-f0005.jpg}
    \caption{Regions that show greater grey matter loss in the aMCI-P group compared to the aMCI-S group (corrected for multiple comparisons, p<0.05). The patterns of cortical atrophy are shown on a 3D surface render (top). In addition the results are shown on a sagittal and coronal slice through the customized template, selected to highlight changes in the cingulate cortex and the medial temporal lobes (bottom). L = left; R = right \cite{whitwellMRIPatternsAtrophy2008}.}
    \label{fig:vbm-metabolic}
\end{figure}


\subsection{VBM as a Prognostic Tool}

Mild Cognitive Impairment (MCI) is heterogeneous.  
VBM can distinguish:

\begin{itemize}
    \item \textbf{MCI converters}: likely to progress to AD
    \item \textbf{MCI non-converters}: may remain stable or revert
\end{itemize}

\cite{bozzaliContributionVoxelbasedMorphometry2006}

\subsection{Differences Between Converters}

MCI converters show atrophy patterns similar to mild AD, involving:

\begin{itemize}
    \item Entorhinal cortex
    \item Hippocampus
    \item Temporal lobe
    \item Parietal lobe
    \item Posterior cingulate cortex
\end{itemize}

Non-converters show more limited and localized GM loss \cite{apostolovaMappingProgressiveBrain2008}.

\subsection{Meta-Analytic Power of VBM}

A major meta-analysis identified the \textbf{left hippocampus and parahippocampal gyrus} as the most consistent predictors of conversion to AD \cite{ferreiraNeurostructuralPredictorsAlzheimers2011}.  
Another found robust atrophy in the \textbf{left amygdala} and \textbf{right hippocampus}.

Rate of Decline in AD

Longitudinal VBM shows that ``fast decliners'' have greater GM loss at baseline, demonstrating that initial atrophy predicts future trajectory \cite{VBMAnticipatesRate}.

Clinical Utility

Tools such as VSRAD provide clinicians with voxel-wise z-scores derived from large normative datasets, similar to how blood test references are used.

Accuracy

VBM often achieves an AUC exceeding 0.90 in distinguishing AD from controls \cite{ishiiVoxelBasedMorphometricComparison2005}, demonstrating high diagnostic accuracy.

\subsection{Subtypes of AD}

VBM helps distinguish:

\begin{itemize}
    \item \textbf{Early-Onset AD (EOAD)} – more parietal and posterior cingulate atrophy
    \item \textbf{Late-Onset AD (LOAD)} – more medial temporal atrophy
\end{itemize}

\cite{ishiiVoxelBasedMorphometricComparison2005a}

Structure–Clinical Correlations

VBM correlates brain atrophy with clinical features:

\begin{itemize}
    \item Frontal atrophy correlates with impaired daily living activities.
    \item Apathy correlates with atrophy in anterior cingulate and orbitofrontal cortex.
    \item Disinhibition correlates with orbitofrontal atrophy.
\end{itemize}

\subsection{Limitations of VBM}

\begin{enumerate}
    \item \textbf{Multiple Comparisons}\\
    Strict corrections (FWE) reduce false positives but may reduce sensitivity.

    \item \textbf{Preprocessing Sensitivity}\\
    Results depend heavily on choices in normalization, segmentation, and statistical thresholds.

    \item \textbf{Registration Problems}\\
    Even DARTEL may misalign regions due to individual variability.

    \item \textbf{Segmentation Errors}\\
    Artifacts, low SNR, and partial volume effects can misclassify tissue.

    \item \textbf{Ambiguity}\\
    A decrease in voxel intensity could reflect thinning, reduced surface area, or folding changes.
\end{enumerate}


\subsection{Machine Learning and VBM}

\begin{enumerate}
    \item Supervised methods (SVM, Random Forests) classify AD vs.\ controls and predict cognitive decline.
    \item Feature importance can reveal novel biomarkers.
\end{enumerate}

\subsection{Deep Learning}

CNNs can classify AD and MCI without hand-crafted features, achieving high accuracy (e.g., 80.9\% for MCI identification) \cite{huangVoxelbasedMorphometryDeep2023}.
